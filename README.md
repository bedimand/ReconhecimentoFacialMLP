# Sistema de Reconhecimento Facial com MLP

Este projeto implementa um sistema completo de reconhecimento facial baseado em uma Rede Neural Perceptron Multicamadas (MLP), contemplando etapas de coleta, pr√©-processamento, treinamento, avalia√ß√£o e infer√™ncia em tempo real.

---

## üìÇ Estrutura do Projeto

- **main.py**            : Ponto de entrada. Exibe menu interativo para executar cada etapa do fluxo.
- **config.yaml**        : Arquivo de configura√ß√£o com par√¢metros (caminhos, tamanho de imagem, hiperpar√¢metros, controles).
- **src/**               : C√≥digo-fonte principal organizado por funcionalidade:
  - **model/**              : Arquivos relacionados ao modelo neural:
    - **model.py**          : Defini√ß√£o da classe `FaceRecognitionMLP`, fun√ß√µes de salvar e carregar modelo.
    - **train.py**          : Fun√ß√£o `train_model` para treinamento com balanceamento de classes e manifest.
    - **evaluation.py**     : Fun√ß√µes de avalia√ß√£o (acur√°cia, relat√≥rio de classifica√ß√£o e matriz de confus√£o).
  - **face/**               : Processamento de faces:
    - **face_detection.py** : Detec√ß√£o via InsightFace e extra√ß√£o de crops centralizados no nariz.
    - **face_recognition.py** : `TransformFactory` para transforma√ß√µes e rotina de infer√™ncia (`recognize_face`).
    - **preprocessing.py**  : M√≥dulo de remo√ß√£o de fundo (MediaPipe) e normaliza√ß√£o.
  - **data/**               : Manipula√ß√£o de dados:
    - **datasets.py**       : Defini√ß√µes de conjuntos de dados e obten√ß√£o de classes.
    - **preprocessing_dataset.py** : Rotina em lote para pr√©-processar dataset completo.
  - **utils/**              : Utilit√°rios diversos:
    - **utils.py**          : Utilit√°rios gerais (console, contagem de imagens, cria√ß√£o de pastas).
    - **visualization.py**  : Desenho de bounding boxes, landmarks, estat√≠sticas e previews.
    - **config.py**         : Carrega `config.yaml` e fornece m√©todos de acesso √†s configura√ß√µes.
    - **capture_faces.py**  : Script para extrair faces de uma pasta de imagens (modo batch).
  - **frame_processor.py**  : Integra detec√ß√£o e reconhecimento em um frame de v√≠deo.

---

## üìù Requisitos

- Python 3.7 ou superior
- Pacotes (via `pip install -r requirements.txt`):
  - opencv-python
  - torch, torchvision
  - insightface, onnxruntime
  - mediapipe
  - pyyaml
  - numpy, Pillow
  - scikit-learn, matplotlib, tqdm

- (Opcional) GPU compat√≠vel com CUDA para acelera√ß√£o

---

## ‚öôÔ∏è Configura√ß√£o

1. Renomeie e adapte `config.yaml` conforme seu ambiente:
   - `paths.dataset_dir`   : Diret√≥rio para armazenar imagens coletadas
   - `paths.model_save_path`: Caminho para salvar o peso do modelo (`.pth`)
   - `image_processing.target_size`: Tamanho (L√óA) do crop de face
   - Hiperpar√¢metros de treinamento (`training.*`)
   - Controles de teclado e cores de visualiza√ß√£o

2. (Opcional) Baixe os modelos do InsightFace em `models/insightface` ou ajuste o `root` no c√≥digo.

---

## üöÄ Fluxo de Uso

Execute:

```bash
python main.py
```

O menu oferece as op√ß√µes:

1. **Detectar e coletar faces (webcam)**
   - Faz detec√ß√£o em tempo real, desenha landmarks e captura crops centralizados no nariz.
   - Aperte tecla configurada para salvar faces individuais ou ative auto-save.

2. **Capturar faces de pasta de imagens**
   - Processa todas as imagens de uma pasta, com ou sem visualiza√ß√£o, salvando crops no dataset.

3. **Treinar modelo de reconhecimento**
   - Pr√©-requisito: execute pr√©-processamento ou tenha `_preprocessed` no dataset.
   - Gera um manifest para separar treino e valida√ß√£o, treina o MLP e salva pesos e classes.

4. **Avaliar modelo de reconhecimento**
   - Carrega o modelo salvo, exclui imagens de treino via manifest e calcula acur√°cia e matriz de confus√£o.

5. **Reconhecimento em tempo real**
   - Captura v√≠deo da webcam, detecta faces, aplica remo√ß√£o de fundo, reconhece usando o MLP e exibe r√≥tulos.

6. **Pr√©-processar e exportar imagem √∫nica**
   - Permite carregar e pr√©-processar um arquivo de imagem, salvando o resultado normalizado.

7. **Pr√©-processar todas as imagens**
   - Executa em lote o pipeline de remo√ß√£o de fundo + resize em todo o dataset.

8. **Sair**

---

## üîç Detalhes T√©cnicos

### Detec√ß√£o e Extra√ß√£o
- InsightFace (`FaceAnalysis`) detecta bounding boxes, 106 landmarks e 5 pontos-chave (kps).
- `extract_face` centraliza o crop no nariz, dimensiona baseado na dist√¢ncia interpupilar e redimensiona ao tamanho configurado.

### Pr√©-processamento
- Offline: `PreprocessModule` usa MediaPipe para remover o fundo antes de salvar imagens em `_preprocessed`.
- Em tempo real: aplica `remove_background_grabcut` antes de normalizar e enviar ao modelo.
- Normaliza√ß√£o: pipeline `ToTensor` + `Normalize` (m√©dias e desvios padr√£o de ImageNet ou configurados).

### Arquitetura do Modelo
- MLP com camadas fully-connected:  input‚Üí2048‚Üí1024‚Üí512‚Üí128‚Üínum_classes.
- Dropout opcional, Adam optimizer, CrossEntropyLoss, scheduler e early stopping configur√°veis.
- Balanceamento de classes via amostragem de inst√¢ncias e manifest.

#### üß† Entendendo o MLP de forma simples

Uma rede MLP (Perceptron Multicamadas) funciona de maneira semelhante ao c√©rebro humano, onde "neur√¥nios artificiais" processam informa√ß√µes em camadas.

**Como funciona nosso reconhecimento facial:**

1. **Transforma√ß√£o da imagem em n√∫meros**:
   - Cada imagem de rosto (128√ó128 pixels) √© convertida em uma longa lista de 49.152 n√∫meros (128√ó128√ó3 canais RGB)
   - √â como "achatar" uma foto colorida em uma √∫nica fileira de n√∫meros

2. **Processamento em camadas**:
   - **Primeira camada (2048 neur√¥nios)**: Recebe os 49.152 n√∫meros e identifica padr√µes b√°sicos (como bordas, contornos)
   - **Camadas intermedi√°rias (1024‚Üí512‚Üí128 neur√¥nios)**: Combinam esses padr√µes em caracter√≠sticas mais complexas (formato dos olhos, nariz, etc.)
   - **Camada final (n√∫mero de pessoas)**: Cada neur√¥nio representa uma pessoa. O que "acender mais forte" indica quem √© reconhecido

3. **Decis√£o por "vota√ß√£o"**:
   - Ap√≥s passar pelos c√°lculos internos, cada neur√¥nio de sa√≠da tem um "valor de ativa√ß√£o"
   - O neur√¥nio com maior valor determina a pessoa reconhecida
   - Esse valor √© convertido em percentual de confian√ßa (ex: "Jo√£o: 92.5%")

**Analogia:** Imagine uma s√©rie de filtros cada vez mais espec√≠ficos. O primeiro filtro separa "√© um rosto?" da imagem inicial. Os filtros seguintes procuram caracter√≠sticas espec√≠ficas: "tem olhos verdes?", "tem nariz fino?", "tem queixo marcado?". A combina√ß√£o √∫nica dessas caracter√≠sticas permite identificar a pessoa espec√≠fica.

**Durante o treinamento**, o sistema aprende automaticamente quais caracter√≠sticas s√£o mais importantes para distinguir cada pessoa no dataset, ajustando milh√µes de "pesos" internos que determinam como os padr√µes s√£o interpretados.

**Na identifica√ß√£o em tempo real**, uma nova imagem percorre o mesmo caminho, e a rede compara seus padr√µes com o que aprendeu anteriormente.

#### Detalhes da Rede MLP

A parte central do sistema √© a Rede Neural Multicamadas (MLP) definida em `src/model/model.py` na classe `FaceRecognitionMLP`. Seus principais aspectos s√£o:

1. **Tamanho da entrada**
   - Cada crop √© redimensionado ao tamanho `(H, W)` configurado em `config.yaml` (`image_processing.target_size`).
   - O tensor resultante tem forma `(C, H, W)` e √© achatado para `(C*H*W)` antes de entrar no MLP.

2. **Camadas fully-connected**
   - `fc1`: `input_size` ‚Üí 2048 unidades
   - `fc2`: 2048 ‚Üí 1024 unidades
   - `fc3`: 1024 ‚Üí 512 unidades
   - `fc4`: 512 ‚Üí 128 unidades
   - `fc5`: 128 ‚Üí `num_classes` (n√∫mero de pessoas + `unknown`)

3. **Fun√ß√£o de ativa√ß√£o**
   - ReLU ap√≥s cada uma das quatro primeiras camadas (`fc1` a `fc4`).
   - Sem ativa√ß√£o na camada de sa√≠da (`fc5`), pois usa logits para `softmax`.

4. **Dropout**
   - Adicionado entre as camadas, se `training.dropout_rate > 0`.
   - Taxa configur√°vel via `config.yaml` (`training.dropout_rate`).

5. **Treinamento**
   - **Perda**: `CrossEntropyLoss` padr√£o.
   - **Otimizador**: Adam, com taxa de aprendizado em `training.learning_rate` (padr√£o: 0.001).
   - **Early stopping**: Configuraado em `training.early_stopping.patience` (padr√£o: 7 √©pocas).
   - **Balanceamento**: Amostragem limitada por classe (max `training.sample_per_class` imagens por pessoa).

6. **Persist√™ncia**
   - Ao final do treinamento, `save_model` grava os pesos em `.pth` e a lista de classes em `.pkl`, criando diret√≥rios automaticamente.

7. **Uso em Infer√™ncia**
   - Em `src/face/face_recognition.py`, o m√©todo `recognize_face` aplica `softmax` sobre os logits para obter probabilidades.
   - Retorna a classe com maior probabilidade e a confian√ßa em porcentagem.

Este design simples e direto permite f√°cil treinamento com os dados coletados, mantendo boa performance de reconhecimento.

### Avalia√ß√£o

#### Resultados

![Matriz de Confus√£o](confusion_matrix.png)

- **Overall accuracy**: 99.33%
- **Detalhes do relat√≥rio de classifica√ß√£o**:

```
              precision    recall  f1-score   support

  anabeatriz       0.97      0.93      0.95       112
    anapaula       0.94      1.00      0.97       130
    bernardo       0.87      0.96      0.91       376
       livia       0.89      0.97      0.93       147
     unknown       1.00      0.99      1.00     14573

    accuracy                           0.99     15338
   macro avg       0.93      0.97      0.95     15338
weighted avg       0.99      0.99      0.99     15338
```

- **Acur√°cia por classe**:
  - anabeatriz: 92.86%
  - anapaula: 100.00%
  - bernardo: 96.28%
  - livia: 96.60%
  - unknown: 99.48%

#### Configura√ß√µes Utilizadas

As configura√ß√µes foram carregadas de `config.yaml`:

```yaml
system:
  python_version: "3.11+"
  device: "cuda"

face_detection:
  model_name: "buffalo_s"
  det_size: [640, 640]
  margin: 20

image_processing:
  target_size: [128, 128]
  normalization:
    mean: [0.5]
    std: [0.5]

training:
  num_epochs: 50
  batch_size: 32
  learning_rate: 0.001
  seed: 42
  validation_split: 0.1
  early_stopping:
    enabled: true
    patience: 7

recognition:
  confidence_threshold: 30.0
  looking_detection:
    enabled: true
    threshold: 0.75

camera:
  width: 1280
  height: 720
  fps: 30
```

### Fluxo de Processamento em Tempo Real

O diagrama abaixo mostra as fun√ß√µes e configura√ß√µes aplicadas em cada etapa do reconhecimento em tempo real:

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [1] CAPTURA                                                               ‚îÇ
‚îÇ - cv2.VideoCapture(0)                                                     ‚îÇ
‚îÇ - cap.set(CAP_PROP_FRAME_WIDTH/HEIGHT, config.get('camera.width/height')) ‚îÇ
‚îÇ - ret, frame = cap.read()                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [2] DETEC√á√ÉO                                                              ‚îÇ
‚îÇ - detect_faces(frame, face_analyzer)                                      ‚îÇ
‚îÇ - InsightFaceFaceAnalysis.get(frame)                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [3] LOOP POR FACE                                                         ‚îÇ
‚îÇ for face in faces:                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [4] EXTRA√á√ÉO                                                              ‚îÇ
‚îÇ - extract_face(frame, face)                                               ‚îÇ
‚îÇ - center_crop_around_nose(...) usando face.bbox e face.kps                ‚îÇ
‚îÇ - redimensiona para config.get('image_processing.target_size')            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [5] PR√â-PROCESSAMENTO                                                     ‚îÇ
‚îÇ - PreprocessModule.remove_background_grabcut(face_img)                    ‚îÇ
‚îÇ   (MediaPipe SelfieSegmentation)                                          ‚îÇ
‚îÇ - TransformFactory.preprocess_face_tensor(face_no_bg, device)             ‚îÇ
‚îÇ   (ToTensor, Normalize(mean,std) do config)                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [6] INFER√äNCIA                                                            ‚îÇ
‚îÇ - recognize_face(model, face_img, classes, device)                        ‚îÇ
‚îÇ - model(face_tensor) + softmax(outputs)                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [7] P√ìS-PROCESSAMENTO                                                     ‚îÇ
‚îÇ - if confidence < config.get('recognition.confidence_threshold'):        ‚îÇ
‚îÇ     result['label']='Desconhecido'                                        ‚îÇ
‚îÇ   else: result['label']=nome + f": {confidence:.1f}%"                   ‚îÇ
‚îÇ - escolhe result['color'] conforme config                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [8] DESENHO                                                               ‚îÇ
‚îÇ - draw_recognition_results(frame, face, result)                           ‚îÇ
‚îÇ - draw_stats(frame, fps, process_time, len(faces), extra_info)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [9] EXIBI√á√ÉO                                                              ‚îÇ
‚îÇ - cv2.imshow("Face Recognition", frame)                                 ‚îÇ
‚îÇ - key = cv2.waitKey(1) para controle de teclas                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Detalhes de cada etapa:**

1. **Captura**: Frame RGB capturado via OpenCV da webcam (configur√°vel para 1280√ó720)
2. **Detec√ß√£o**: InsightFace detecta faces, retornando bounding boxes e landmarks (106 + 5 pontos-chave)
3. **Loop de faces**: Cada face detectada √© processada separadamente
4. **Extra√ß√£o**: Recorte da face centralizado no nariz usando dist√¢ncia interpupilar para escala
5. **Pr√©-processamento**:
   - Remo√ß√£o de fundo via MediaPipe SelfieSegmentation
   - Convers√£o para tensor PyTorch
   - Normaliza√ß√£o usando m√©dias/desvios do ImageNet
   - Adi√ß√£o da dimens√£o de lote (batch)
6. **Infer√™ncia**: Forward pass no MLP seguido de softmax para obter probabilidades
7. **P√≥s-processamento**:
   - Verifica√ß√£o contra threshold de confian√ßa (`recognition.confidence_threshold`)
   - Decis√£o entre "unknown" e pessoa reconhecida
   - Formata√ß√£o da etiqueta com nome e porcentagem
8. **Visualiza√ß√£o**: Desenho de caixa delimitadora, r√≥tulo e informa√ß√µes de estat√≠sticas no frame
9. **Exibi√ß√£o**: Apresenta√ß√£o do frame processado com anota√ß√µes ao usu√°rio

Este fluxo executa em tempo real em um loop cont√≠nuo, processando cada frame capturado.
